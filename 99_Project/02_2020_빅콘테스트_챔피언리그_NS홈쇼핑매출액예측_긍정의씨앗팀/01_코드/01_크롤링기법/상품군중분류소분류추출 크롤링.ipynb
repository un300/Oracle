{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "상품군중분류소분류.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2Si0_iCs_9-"
      },
      "source": [
        "# 빅 콘데스트\n",
        "2020 9 28"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaiaVdEmtLPo"
      },
      "source": [
        "*팀명:긍정의씨앗*\n",
        "\n",
        "\n",
        "팀원 : 이재원, 박세진, 최우진, 최호진, 황인범\n",
        "챔피언스리그 참가"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdHGNcaUpmY6"
      },
      "source": [
        "# **중분류/소분류 생성**\n",
        "\n",
        "\n",
        "* 원데이터의 '상품군'을 좀 더 세세하게 집단으로 쪼개서 분석 할 수 있는 방법이 없을까?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnkwBlvMsfL0"
      },
      "source": [
        "## 1. NS SHOP+ 홈페이지 크롤링\n",
        "\n",
        "\n",
        "\n",
        "* 원데이터 상품명에 대한 중분류와 소분류 카테고리 정보를 CSV파일로 가져오기\n",
        "\n",
        "\n",
        "* NS SHOP+ 홈페이지 주소 : https://www.nsmall.com/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dB94wvUqw2fJ"
      },
      "source": [
        "### 크롤링을 위한 패키지 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Kvbq52NwHxV"
      },
      "source": [
        "from urllib.parse import quote_plus\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait   \n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.common.exceptions import TimeoutException   \n",
        "import time\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import random\n",
        "\n",
        "#  Chrome webdriver 를 읽는다.\n",
        "wd = webdriver.Chrome('C:\\\\Users\\\\hwang in beom\\\\Desktop\\\\chromedriver.exe')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbBx6sq5yhxi"
      },
      "source": [
        "### 웹드라이버로 페이지 이동하며 크롤링"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mPerHjAwNIk"
      },
      "source": [
        "count = 0\n",
        "\n",
        "\n",
        "# 해당 페이지에서  script를 사용해 페이지 버튼을 클릭하면서 소분류의 데이터를 크롤링 한 후 다음 페이지로 이동하는 부분\n",
        "def crawling_goobne(url, main_category, middle_category, subclass_category ,wd, count):\n",
        "    wd.get(url)\n",
        "\n",
        "    RESULT_DIRECTORY = '../'\n",
        "    results = []\n",
        "    list_name = []\n",
        "    if len(last) == 0:\n",
        "        print('none')\n",
        "    else:\n",
        "        for i in range(1, int(last[0])+1):\n",
        "            script = 'movePage(%d)' % i\n",
        "            wd.execute_script(script)  # js 실행\n",
        "            time.sleep(2 + random.uniform(0.01, 0.1))              \n",
        "\n",
        "            html = wd.page_source\n",
        "            bs = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "            tags = bs.findAll('span', attrs={'class': 'inform pr10'})\n",
        "            print(tags)\n",
        "            for tag in tags:\n",
        "                list_name.append(tag.get_text())\n",
        "        count += 1\n",
        "        table = pd.DataFrame(list_name)\n",
        "        print(table)\n",
        "        table.to_csv('./data/{}-{}-{}.csv'.format(main_category, middle_category, subclass_category), encoding='euc-kr', mode='w', errors=None,)\n",
        "        time.sleep(5 + random.uniform(0.01, 0.1)) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WA_WrANHzCKJ"
      },
      "source": [
        "### BeautifulSoup 이용해 소분류 url 가져오기\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvcsgQYowVq_"
      },
      "source": [
        "# nsmall 해당 페이지로 이동\n",
        "url_str ='http://www.nsmall.com/CategoryDisplay?searchType=search&storeId=13001&langId=-9&catalogId=97001&categoryId=1583591&cate1Code=1583596&cate2Code=1583567&cate3Code=1583591&menuUri=NSSubCategoryPageLayoutView&selfId='\n",
        "req = requests.get(url_str)\n",
        "\n",
        "# HTML 소스를 BeautifulSoup를 통해 가져오기\n",
        "html = req.text\n",
        "bs = BeautifulSoup(html, 'html.parser')\n",
        "url_list = bs.find('li', attrs={'class': 'cate_List third'}).find('ul', attrs={'class': 'depth1'}).find_all(\"li\")\n",
        "\n",
        "\n",
        "# 해당 카테고리의 중분류 url 가져오기 \n",
        "url_list_subclass = []\n",
        "for i in range(0,len(url_list)):\n",
        "    start = (str(url_list[i]).find('href=')) + 6\n",
        "    end = str(url_list[i]).find('\"><span>')\n",
        "    url_list[i] = (\"http://www.nsmall.com/\" + str(url_list[i])[start:end]).replace('amp;', '')\n",
        "\n",
        "    req = requests.get(url_list[i])  \n",
        "    html = req.text\n",
        "    bs = BeautifulSoup(html, 'html.parser')\n",
        "    url_list_subclass.append(bs.find('dl', attrs={'class': 'lst hnl'}).find_all(\"li\"))\n",
        "\n",
        "# 해당 중분류의 소분류 카테고리의 url 가져오기\n",
        "subclass_url_list = []\n",
        "for i in url_list_subclass:\n",
        "    count = 0\n",
        "    for j in i:\n",
        "        if count == 0:\n",
        "            count += 1\n",
        "            continue\n",
        "        start = (str(j).find('<a href=')) + 9\n",
        "        j = (str(j)[int(start):])\n",
        "        end = str(j).find('\" id')\n",
        "        subclass_url_list.append((\"http://www.nsmall.com/\" + str(j)[:end]).replace('amp;', ''))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrThmxyi0Bxx"
      },
      "source": [
        "### 소분류 url을 통해 상품명에 따른 카테고리 정보 가져오기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Y-XOfo0phDh"
      },
      "source": [
        "# 해당 소분류의 url을 통해 각 상품의 상품군/중분류/소분류를 가져오는 부분\n",
        "for i in subclass_url_list:\n",
        "    if count < 6:\n",
        "        count += 1\n",
        "        continue\n",
        "    url_str = i\n",
        "    print(url_str)\n",
        "    req = requests.get(url_str)\n",
        "    ## HTML 소스 가져오기\n",
        "    html = req.text\n",
        "    # print(html)\n",
        "    bs = BeautifulSoup(html, 'html.parser')\n",
        "    pagging = bs.find('div', attrs={'class': 'lst_paging'})\n",
        "    last = re.findall(\"\\d+\", str(pagging.find('a', attrs={'class': 'last'})))\n",
        "    main_category = bs.find('li', attrs={'class': 'cate_List second'}).find('a', attrs={'href': '#;'}).get_text()\n",
        "    middle_category = bs.find('li', attrs={'class': 'cate_List third'}).find('a', attrs={'href': '#;'}).get_text()\n",
        "    subclass_category = bs.find('div', attrs={'class': 'soCate'}).find('strong').get_text()+str(count)\n",
        "\n",
        "    main_category = main_category.replace('/', ',')\n",
        "    middle_category = middle_category.replace('/', ',')\n",
        "    subclass_category = subclass_category.replace('/', ',')\n",
        "    crawling_goobne(url_str, main_category, middle_category, subclass_category, wd, count)\n",
        "    count += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWP8N3P1zsg7"
      },
      "source": [
        "## 2. 상품에 대한 해당 카테고리 가져오기\n",
        "\n",
        "\n",
        "* 상품명의 특정 단어를 검색하여, NS SHOP+ 홈페이지에서 정의된 중분류, 소분류 카테고리 정보 가져오는 알고리즘 생성\n",
        "\n",
        "* ex) \"그릴\" 검색  => 상품군 : 주방 / 중분류 : 주방가전 / 소분류 : 조리,가열기기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-zoxouT0Uod"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# 해당 중분류의 csv 파일을 가져오는 작업\n",
        "\n",
        "while(1):\n",
        "    find_class = input(\"찾으실 분류 이름을 입력하세요 :\")\n",
        "    if find_class == '0':\n",
        "        break\n",
        "    df = pd.read_csv(\"data/\" + find_class +\"-소분류.csv\")\n",
        "    coulmn = df.columns\n",
        "\n",
        "    # 소분류로 분류 된 값들을 이름으로 검색 하는 작업\n",
        "    while(1):\n",
        "        find_name = input(\"찾을 이름을 입력하세요 :\")\n",
        "        if find_name == '0':\n",
        "            break\n",
        "        result = False\n",
        "        for i in coulmn:\n",
        "            if find_class not in i:\n",
        "                continue\n",
        "            np_data = np.array(df[i].tolist())\n",
        "            list_data = list(np_data)\n",
        "            for j in list_data:\n",
        "                if find_name in j:\n",
        "                    result = True\n",
        "                    break\n",
        "            if result == True:\n",
        "                print(\"******\")\n",
        "                print(i)\n",
        "                print(\"******\")\n",
        "                result = False\n",
        "\n",
        "print(\"고생하셨습니다\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}