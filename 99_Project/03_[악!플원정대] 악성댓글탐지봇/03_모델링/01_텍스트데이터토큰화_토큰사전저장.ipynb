{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"01_텍스트데이터토큰화 & 토큰사전저장","provenance":[],"collapsed_sections":[],"mount_file_id":"1uWyj3bvnvNNVhlnVL25U59-5YHkZepyq","authorship_tag":"ABX9TyOLQlkJ3NBgYjO/fNUuH5u4"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"_W15hoUn_-cb"},"source":["###### 2020-11-22 일요일"]},{"cell_type":"markdown","metadata":{"id":"IfiAG8oG_-Xw"},"source":["# 01_텍스트데이터토큰화 & 토큰사전저장"]},{"cell_type":"code","metadata":{"id":"uJINtzqm0jQJ"},"source":["import warnings\n","warnings.filterwarnings(action='ignore')\n","\n","import pandas as pd\n","import numpy as np\n","import re\n","import json\n","\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VssFkduo0e5O"},"source":["## 1. 데이터 불러오기"]},{"cell_type":"code","metadata":{"id":"9JT1UAar0ee9"},"source":["d1 = pd.read_csv('/content/drive/MyDrive/[final_project]_악플원정대/data/division_data1(2020-11-17).csv')\n","d2 = pd.read_csv('/content/drive/MyDrive/[final_project]_악플원정대/data/division_data2(2020-11-17).csv')\n","# division_data3 경우는 인코딩안해주면 에러뜸 ㅠ\n","d3 = pd.read_csv('/content/drive/MyDrive/[final_project]_악플원정대/data/division_data3(2020-11-17).csv', encoding='cp949')\n","# division_data4 경우는 read_excel로 불러와야 에러안뜸 ㅠ\n","d5 = pd.read_excel('/content/drive/MyDrive/[final_project]_악플원정대/data/division_data5(2020-11-17).csv')\n","d6 = pd.read_csv('/content/drive/MyDrive/[final_project]_악플원정대/data/division_data6(재원).csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"p6AgI_gJ0ede"},"source":["df_list = [d1, d2, d3, d5, d6]\n","\n","all_df = d1\n","for df in df_list[1:]:\n","    all_df = pd.concat([all_df, df])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_1kskc4N0ebA"},"source":["all_df.drop(['Unnamed: 0', '출처'], axis=1, inplace=True)\n","\n","index = all_df['악플여부'].isna()\n","raw_df = all_df[~index]\n","\n","raw_df = raw_df.reset_index()\n","raw_df.drop(['index'], inplace=True, axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Oo5NpExR0eYV"},"source":["copy_df = raw_df.copy()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cTN2x4wV0eWQ"},"source":["# 고정 시드값 지정\n","seed = 123\n","\n","# 댓글 길이 지정\n","comment_len = 400"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jxbEw-KV1JZ7"},"source":["##### 악플여부 라벨이 1인경우 제거\n"," - 악플여부 라벨이 1인경우는 사람에 따라 불쾌할수도 있고 아닐수도 있기 때문에 기준을 매기기가 애매하다. 그러므로 객관적인 기준을 세울 수 없기 때문에 제거한다"]},{"cell_type":"code","metadata":{"id":"rOOsFOAk0eT-"},"source":["index = copy_df['악플여부'] == 1\n","copy_df = copy_df[~index]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ey0s5QsW0eRh"},"source":["index = copy_df['악플여부'] == 2\n","copy_df.loc[index, '악플여부'] = 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q5nFXMKK0ePL"},"source":["copy_df['악플여부'] = copy_df['악플여부'].apply(int)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mR-V2bM90eNF"},"source":["copy_df['악플여부'].value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l0Zf1EnX0eKe"},"source":["copy_df['댓글'] = copy_df['댓글'].apply(str)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dPU6IaTY1Ye2"},"source":["## 2. 토큰화 & 저장\n"," - Train set, Test set을 나눈다\n"," - Train set을 기준으로 음절 토큰 사전을 만든다\n"," - Train set / Test set / 음절토큰사전을 저장한다\n","    - 이렇게 저장하는 이유는? : 매번 음절토큰사전을 만드는데 시간이 많이 소요되기 때문에 한번 만들어놓고 재사용하여 시간을 절약!\n"]},{"cell_type":"code","metadata":{"id":"suVm1lU-1rPf"},"source":["def tokenization_save(df):\n","\n","    feature = df['댓글']\n","    label = df['악플여부']\n","\n","    X_train, X_test, y_train, y_test = train_test_split(feature, label, \n","                                                    test_size=0.2,\n","                                                    random_state=seed)\n","    X_train_split = X_train.apply(list).tolist()\n","    X_train_token_list = sum(X_train_split, [])\n","    \n","    tokenizer = Tokenizer()\n","    tokenizer.fit_on_texts(X_train_token_list)\n","    \n","    word_index_vocab = tokenizer.word_index\n","\n","    return X_train, X_test, y_train, y_test, word_index_vocab\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gJXm_yRQ0eFz"},"source":["def train_test_save(X_train, X_test, y_train, y_test) :\n","    save_df_list = [X_train, X_test, y_train, y_test]\n","    file_names = ['X_train', 'X_test', 'y_train', 'y_test']\n","    for idx, set in enumerate(save_df_list):\n","        set.to_csv('/content/drive/MyDrive/[final_project]_악플원정대/data/' + file_names[idx] + '.csv', encoding='UTF-8_SIG')\n","\n","\n","def save_word_index(word_index_vocab) :\n","    json_file = json.dumps(word_index_vocab, ensure_ascii=False)\n","    f = open('/content/drive/MyDrive/[final_project]_악플원정대/data/word_index_vocab.json', 'w')\n","    f.write(json_file)\n","    f.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1uW4OC9F0eIK"},"source":["X_train, X_test, y_train, y_test, word_index_vocab = tokenization_save(copy_df)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RNTpftu3BsvP"},"source":["# ' '(공백)도 사전에 포함 \n","word_index_vocab[' '] = len(word_index_vocab) + 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S0Mmkk900eDY"},"source":["train_test_save(X_train, X_test, y_train, y_test)\n","save_word_index(word_index_vocab)"],"execution_count":null,"outputs":[]}]}