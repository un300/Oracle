{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2020-10-22 목요일"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 토큰화(Tokenization)\n",
    "\n",
    "\n",
    " - 자연어 처리에 있어 텍스트 전처리는 매우 중요한 작업이다.\n",
    " - 용도에 맞게 텍스트를 처리하여야 한다.\n",
    " - 크롤링 등으로 얻어낸 텍스트를 용도에 맞게 사용하려면 다음과 같은 과정을 진행해야 한다.\n",
    "      1. 토큰화 (Tokenization)\n",
    "      2. 정제 (Cleaning)\n",
    "      3. 정규화 (Nomalization)\n",
    " - 먼저 토큰화를 알아보자\n",
    " - 토큰화는 간략히 말하자면, 코퍼스(Corpus)라 불리는 택스트 덩어리를 토큰(Token)이라 불리는 단위로 나누는 작업을 말한다.\n",
    " - 토큰은 상황에 따라 의미가 다르지만, 보통 의미있는 단위로 토큰을 정의한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - 목차\n",
    "\n",
    "## 1. 토큰화(Tokenization)\n",
    "   - #### 1-1 단어 토큰화(Word Tokenization)\n",
    "   - #### 1-2 토큰화 중 생기는 선택의 순간\n",
    "   - #### 1-3 토큰화에서 고려해야할 사항\n",
    "   - #### 1-4 문장 토큰화(Sentence Tokenization)\n",
    "   - #### 1-5 이진 분류기(Binary Classifier)\n",
    "   - #### 1-6 한국어에서의 토큰화의 어려움\n",
    "   - #### 1-7 품사 태깅(Part of speech tagging)\n",
    "   - #### 1-8 NLTK와 KoNLPy를 이용한 영어, 한국어 토큰화 실습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-1 단어 토큰화 (Word Tokenization)\n",
    "   - 토큰의 기준을 **단어**로 하는 경우 `단어 토큰화(Word Tokenization)`이라 한다. 여기서 단어는 단어단위 외에도 단어구, 의미를 갖는 문자열로도 간주되기도 한다.\n",
    "   \n",
    "##### 예를들어 아래의 입력으로 구두점(반점, 온점, 따옴표 등등..)과 같은 문자는 제외시키는 간단한 단어 토큰화 작업을 보자.\n",
    "- `Time is an illusion. Lunchtime double so!`\n",
    "- 출력 : \"Time\" / \"is\" / \"an\" / \"illustion\" / \"Lunchtime\" / \"double\" / \"so\"\n",
    "        구두점을 제외하고 whitespace를 기준으로 잘라낸 결과이다.\n",
    "        이 예제는 토큰화의 가장 기초적인 예제에 불과하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-2 토큰화 중 생기는 선택의 순간\n",
    "   - 토큰화를 하다보면, 예상하지 못한 경우가 있어서 **토큰화의 기준**을 생각해봐야 하는 경우가 발생한다.\n",
    "   - 예를들어 영어의 경우 어포스트로피(')가 들어가 있는 단어는 토큰화의 기준을 어떻게 정의해야할지 정해야한다.\n",
    "   \n",
    "##### 예를들어보자\n",
    "   - `Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.`\n",
    "   \n",
    "   \n",
    "   **위 문장에서 `Don't`와 `Jone's`는 어떻게 토큰화해야하는가?**\n",
    "   - 아래의 예시를 보자.\n",
    "   제공하는 패키지마다 **토큰화의 기준**이 다름을 알 수 있다.\n",
    "  \n",
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 아래의 `word_tokenize`는 `Don't`를 `Do`와 `n't`로 구분하고 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Do', \"n't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.', 'Jone', \"'s\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize  \n",
    "print(word_tokenize(text))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 아래의 `WordPunctTokenizer`는 `Don't`를 `Don`과 `'`,  `t`로 구분하고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Don', \"'\", 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr', '.', 'Jone', \"'\", 's', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer  \n",
    "print(WordPunctTokenizer().tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### keras가 제공하는 `text_to_word_sequence`는 `'`를 구분하지 않는다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lan41\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\lan41\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\lan41\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\lan41\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\lan41\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\lan41\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\lan41\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\lan41\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\lan41\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\lan41\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\lan41\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\lan41\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"don't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', 'mr', \"jone's\", 'orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "print(text_to_word_sequence(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-3 토큰화에서 고려해야할 사항\n",
    "   - **토큰화의 작업**을 단순히 코퍼스에서 구두점을 제거하고 whitespace를 기준으로 나누는 작업이라고 간주할 수는 없다. \n",
    "   - **토큰화 작업**은 섬세한 알고리즘이 필요한데, `왜 섬세해야 하는가?` 를 알아보자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) 구두점이나 특수 문자를 단순 제외해서는 안 된다.\n",
    "   - 코퍼스에서 구두점이나 특수문자를 단순히 제외하는 것은 옳지 않다. \n",
    "   \n",
    "   \n",
    "   #### 다음예를보자\n",
    "   \n",
    "   \n",
    "   `Ph.D` / `$45.55`    >  이것들은 `.`이나 `$`가 사라지면 의미가 사라진다!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) 줄임말과 단어 내에 띄어쓰기가 있는 경우\n",
    "   - 영어권의 경우 `'`는 줄임말을 나타내기도 한다. (ex : `I'm` == `I am`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) 표준 토큰화 예제\n",
    "   - 표준으로 쓰이고 있는 토큰화 방법 중 하나인 `Penn Treebank Tokenization`의 규칙을 알아보고, 토큰화의 결과를 보도록 하자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    규칙 1. 하이푼으로 구성된 단어는 하나로 유지한다.\n",
    "    규칙 2. doesn't와 같이 아포스트로피로 '접어'가 함께하는 단어는 분리해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Starting a home-based restaurant may be an ideal. it doesn't have a food chain or restaurant of their own.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Starting', 'a', 'home-based', 'restaurant', 'may', 'be', 'an', 'ideal.', 'it', 'does', \"n't\", 'have', 'a', 'food', 'chain', 'or', 'restaurant', 'of', 'their', 'own', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer=TreebankWordTokenizer()\n",
    "print(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-4 문장 토큰화(Sentence Tokenization)\n",
    "   - 이번에는 토큰화의 단위가 문장(sentence)일 때, 어떻게 토큰화를 수행해야할지 알아보자. \n",
    "   - 이 작업은 코퍼스를 문장 단위로 구분하는 작업이다. \n",
    "   - 때로는 `문장 분류(sentence segmentation)`이라고 한다.\n",
    "   \n",
    "   \n",
    "   \n",
    "#### 그렇다면 문장을 구분하는 기준은??  :  온점`.` / 반점`,` 등으로 구분하면 되지 않을까?\n",
    "   - 아주 잘못된 생각!\n",
    "   - 다음예제를 보자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - `EX1)` IP 192.168.56.31 서버에 들어가서 로그 파일 저장해서 lan4148@naver.com으로 결과 좀 보내줘. 그러고나서 점심 먹으러 가자.\n",
    "\n",
    " - `EX2)` Since I'm actively looking for Ph.D. students, I get the same question a dozen times every year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`EX1)`의 경우 `IP 192.168.56.31`에서 반점이 포함되어 있어, 반점으로 문장을 토큰화하면 올바른 토큰화가 되지 않음!\n",
    "\n",
    "\n",
    "`EX2)`의 경우 `Ph.D.`에서 반점이 포함되어 있어, 반점으로 문장을 토큰화하면 올바른 토큰화가 이루어지지 않음!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 `nltk` 패키지에서 제공하는 `sent_tokenize`함수를 통해 실습해보자\n",
    "\n",
    "아래의 결과를 보면 성공적으로 모든 문장을 구분해 내었음을 알 수있다.\n",
    "\n",
    "`nltk`는 단순히 온점을 구분자로 하여 문장을 구분하지 않기 때문에\n",
    "\n",
    "'Ph.D.'를 문장 내의 단어로 인식하여 성공적으로 인식하는 것을 볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"His barber kept his word. But keeping such a huge secret to himself was driving him crazy. Finally, the barber went up a mountain and almost to the edge of a cliff. He dug a hole in the midst of some reeds. He looked about, to mae sure no one was near.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['His barber kept his word.', 'But keeping such a huge secret to himself was driving him crazy.', 'Finally, the barber went up a mountain and almost to the edge of a cliff.', 'He dug a hole in the midst of some reeds.', 'He looked about, to mae sure no one was near.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 한국어의 문장 토큰화를 실습해보자\n",
    "   - 한국어의 토큰화를 위한 여러가지 토큰화 도구가 존재하지만, 박상길 님이 개발한 `KSS(Korean Sentence Splitter)`를 사용해보자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KSS 설치가 안되네요 ㅠㅠ.. 빠른 시일 내에 설치해 보겠습니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-5 이진 분류기(Binary Classifier)\n",
    "   - 문장 토큰화에서 예외 사항을 발생시키는 온점을 처리하기 위해서, 입력에 따라 두 개의 클래스로 분류하는 이진 분류기(Binary Classifier)를 사용하기도 한다.\n",
    "   - 여기서 말하는 두 개의 클래스는 다음과 같다\n",
    "       1. 온점(.)이 단어의 일부분인 경우 ex Ph.D\n",
    "       2. 온점(.)이 정말로 문장의 끝을 나타내는 경우\n",
    "\n",
    "   - **이진분류기**는 특정한 규측을 `함수(def)`형태로 구현해 놓은 것일 수도 있고, 머신러닝의 `분류(classification)`를 통해 **이진분류기**를 구축하기도 한다.\n",
    "   \n",
    "   \n",
    "##### 그렇기 때문에 영어권에서는!\n",
    "   - **이진분류기**를 구현하기 위해 온점(.)이 약어인지 아닌지 `약어사전`을 통해 확인한다.\n",
    "   - `https://public.oed.com/how-to-use-the-oed/abbreviations/` 다음링크는 약어사전의 예이다\n",
    "   \n",
    "   \n",
    "##### 이러한 문장 토큰화를 수행하는 오픈소스로는!\n",
    "   -  NLTK, OpenNLP, 스탠포드 CoreNLP, splitta, LingPipe 등이 있다.\n",
    "   - **문장 토큰화**를 진행할 때, 문장 토큰화의 규칙을 짜면서 발생할 수 있는 여러가지 예외사항은 다음 사이트를 참조하라\n",
    "   `https://www.grammarly.com/blog/engineering/how-to-split-sentences/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-6 한국어에서의 토큰화의 어려움\n",
    "   - 영어는 New York과 같은 함성어나 he's와 같이 줄임말에 대한 규칙을 잘 지정한다면, 대부분 whitespace(공백)을 기준으로 **문장토큰화**를 수행해도 문제없이 작동한다.\n",
    "   - 하지만 한국어의 경우 whitespace(공백)만으로 **문장토큰화**를 수행할 수 없다\n",
    "   - 왜냐하면, 한국어는 단어에 조사, 어미등을 붙여서 만드는 **교착어**이기 때문이다.\n",
    "       예를들어 `지수는` 에서 `지수 + 는`을 뜻한다.\n",
    "       \n",
    "       \n",
    "###### 그래서 한국어의 자연어처리는 **형태소(morpheme)**이란 개념을 반드시 이해하여야 한다!\n",
    "\n",
    "### 1) 한국어는 교착어이다\n",
    "   - **형태소**란, 뜻을 가진 가장 작은 말의 단위를 말한다.\n",
    "   - **자립형태소** : 접사 / 어미 / 조사와 상관없이 자립하여 사용할수 있는 형태소\n",
    "   - **의존형태소** : 다른 형태소와 결합하여 사용되는 형태소\n",
    "   - `나는 딥러닝책을 읽었다`의 예를 들면\n",
    "       1. 자립형태소 : `나` / `딥러닝책`\n",
    "       2. 의존형태소 : `-는` / `-을` / `읽-` / `-었-` / `-다`\n",
    "       \n",
    "       \n",
    "### 2) 한국어는 띄어쓰기가 영어보다 잘 지켜지지 않는다\n",
    "   - 영어의 경우 띄어쓰기를 하지 않으면 영어의 의미를 손쉽게 알아차리지 못해 띄어쓰기를 철저히 지킨다.\n",
    "       - Tobeornottobethatisthequestion\n",
    "   - 하지만 한국어의 경우는 뜨어쓰기가 무시되어도 그 의미를 쉽게 잃지 않는다.\n",
    "       - 제가이렇게띄어쓰기를전혀하지않고글을썼다고하더라도글을이해할수있습니다.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-7 품사 태깅(Part of speech tagging)\n",
    "   - 단어의 표기는 같지만 의미가 다른 단어가 있다. \n",
    "   - `못`의 예를들어보자\n",
    "       - 명사로써는 `망치를 이용해 목재따위를 고정하는 물체`를 뜻 한다.\n",
    "       - 부사고써는 `동작 동사를 부정하는 의미`를 뜻 한다.\n",
    "   - 위와 같은 경우를 고려하기 위해, 토큰화 작업시 품사를 표시하도록 하는데 그것을 **품사태깅** 이라고한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-8 `NLTK`와 `KoNLPy`를 이용한 영어, 한국어 토큰화 실습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - NLTK에서는 영어 코퍼스에 **품사태깅** 기능을 제공하고 있다. \n",
    " - **품사태깅**에는 여러가지 기준이 있는데, NLTK는 `Penn Treebank POS Tags`라는 기준을 사용한다.\n",
    " - NLTK의 **품사태깅** 기능을 실습해보자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (1) `NLTK` 패키지를 이용한 영어 토큰화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Penn Treebank POG Tags\n",
    "   - PRP    : 인칭 대명사\n",
    "   - VBP    : 동사\n",
    "   - RB     : 부사 \n",
    "   - VBG    : 현재부사 \n",
    "   - IN     : 전치사\n",
    "   - NNP    : 고유 명사 \n",
    "   - NNS    : 복수형 명사 \n",
    "   - CC     : 접속사\n",
    "   - DT     :관사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I am actively looking for Ph.D. students. and you are a Ph.D. student.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'actively', 'looking', 'for', 'Ph.D.', 'students', '.', 'and', 'you', 'are', 'a', 'Ph.D.', 'student', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\lan41\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " ('am', 'VBP'),\n",
       " ('actively', 'RB'),\n",
       " ('looking', 'VBG'),\n",
       " ('for', 'IN'),\n",
       " ('Ph.D.', 'NNP'),\n",
       " ('students', 'NNS'),\n",
       " ('.', '.'),\n",
       " ('and', 'CC'),\n",
       " ('you', 'PRP'),\n",
       " ('are', 'VBP'),\n",
       " ('a', 'DT'),\n",
       " ('Ph.D.', 'NNP'),\n",
       " ('student', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize    # word_tokenize : 단어기준으로 토큰화를 진행해주는 함수\n",
    "from nltk.tag import pos_tag               # pos_tag : 토큰들의 품사를 지정해주는 함수\n",
    "\n",
    "\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')  # nlik패키지는 필요한 함수를 사용하려면 이렇게 다운받아야 하드라구여?\n",
    "\n",
    "token = word_tokenize(text)   # 단어기준으로 토큰화 진행\n",
    "\n",
    "print(token)     # 한번 출력해주고\n",
    "pos_tag(token)   # 토큰들에 품사를 붙여주는 작업 진행\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (2) `KoNLPy` 프레임워크를 이용한 한국어 토큰화\n",
    "   - `KoNLPy`프레임워크는 형태소분석(영어의 토큰화와 같은의미)을 위해서 다양한 형태소 분석기를 제공한다\n",
    "   - `Okt(Open Korea Text)`, `메캅(Mecab)`, `코모란(Komoran)`, `한나눔(Hannanum)`, `꼬꼬마(Kkma)`\n",
    "   - 한국어에서 **형태소분석**이란, 형태소 단위로 토큰화를 진행하는 것을 의미한다.\n",
    "   - `Okt`와 `Kkma`를 이용해 한국어 형태소분석 실습을 진행해보자\n",
    "   - `Okt`는 기존엔 `Twitter`라는 이름을 가지고 있었다. (0.5.0 버전부터 이름 바뀜)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### - Okt를 이용한 한국어 형태소분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"열심히 일한 당신, 연휴에는 여행을 가봐요\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< 형태소 분석 >\n",
      "['열심히', '일', '한', '당신', ',', '연휴', '에는', '여행', '을', '가봐요']\n",
      "\n",
      "< 형태소 분석 + 품사태깅 >\n",
      "[('열심히', 'Adverb'), ('일', 'Noun'), ('한', 'Josa'), ('당신', 'Noun'), (',', 'Punctuation'), ('연휴', 'Noun'), ('에는', 'Josa'), ('여행', 'Noun'), ('을', 'Josa'), ('가봐요', 'Verb')]\n",
      "\n",
      "< 명사 추출 >\n",
      "['일', '당신', '연휴', '여행']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt   # konlpy에서 okt 불러오기\n",
    "okt = Okt()\n",
    "\n",
    "print('< 형태소 분석 >')\n",
    "print(okt.morphs(text))\n",
    "print()\n",
    "\n",
    "print('< 형태소 분석 + 품사태깅 >')\n",
    "print(okt.pos(text))\n",
    "print()\n",
    "\n",
    "print('< 명사 추출 >')\n",
    "print(okt.nouns(text))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### - Kkma를 이용한 한국어 형태소분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"열심히 일한 당신, 연휴에는 여행을 가봐요\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< 형태소 분석 >\n",
      "['열심히', '일한', '당신', ',', '연휴', '에', '는', '여행', '을', '가보', '아요']\n",
      "\n",
      "< 형태소분석 + 품사태깅 >\n",
      "[('열심히', 'MAG'), ('일한', 'NNG'), ('당신', 'NP'), (',', 'SP'), ('연휴', 'NNG'), ('에', 'JKM'), ('는', 'JX'), ('여행', 'NNG'), ('을', 'JKO'), ('가보', 'VV'), ('아요', 'EFN')]\n",
      "\n",
      "< 명사 추출 >\n",
      "['일한', '당신', '연휴', '여행']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Kkma  # konlpy에서 Kkma 불러오기\n",
    "kkma = Kkma()\n",
    "\n",
    "print('< 형태소 분석 >')\n",
    "print(kkma.morphs(text))\n",
    "print()\n",
    "\n",
    "print('< 형태소분석 + 품사태깅 >')\n",
    "print(kkma.pos(text))\n",
    "print()\n",
    "\n",
    "print('< 명사 추출 >')\n",
    "print(kkma.nouns(text))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - 한국어 형태소 분석기 중에서 `okt`와 `kkma`만을 사용해봤지만, 두 형태소 분석기의 결과가 다름을 알 수 있다\n",
    " - 그러므로 자기가 사용하고자 하는 필요용도에 따라 적절한 형태소 분석기를 선택하여야 한다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
