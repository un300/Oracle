{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2020-10-29 목요일"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07_패딩(Padding)\n",
    "   - 자연어 처리를 하다보면 각 문장을 토큰화 하는데, 문장을 단어로 토큰화 할때, 문장마다 서로 길이가 다를 수 있다.\n",
    "   - 문장마다 토큰의 개수가 다르게 생성된다. 이말은 각 문장마다 가지고 있는 데이터의 수가 다르다(길이가 다르다)는 말과 같다 \n",
    "   - 기계는 행렬(또는 데이터프레임) 형태로 데이터를 처리하는 것이 편하기 때문에, 단어 토큰화된 토큰들을 행렬 형태로 만들어 주어야 한다.\n",
    "   - 그러므로 행렬 연산을 수행하기 위해 길이가 짧은 데이터에 임의의 숫자(0)를 넣어 행렬의 모양을 동일하게 만들어 주는 작업을 **패딩**이라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Numpy로 패딩하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 06_정수인코딩(Integer Encoding)에서 했던 예제를 이용하여 패딩을 실습해보자\n",
    "##### 우선 문장을 토큰화 해야하는데 연습하는 셈 치고 다시한번 해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"A barber is a person. a barber is good person. a barber is huge person. he Knew A Secret! The Secret He Kept is huge secret. Huge secret. His barber kept his word. a barber kept his word. His barber kept his secret. But keeping and keeping such a huge secret to himself was driving the barber crazy. the barber went up a huge mountain.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['Knew', 'Secret'], ['The', 'Secret', 'Kept', 'huge', 'secret'], ['Huge', 'secret'], ['His', 'barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['His', 'barber', 'kept', 'secret'], ['But', 'keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n"
     ]
    }
   ],
   "source": [
    "# 텍스트데이터 문장 토큰화 하기\n",
    "sent_token = sent_tokenize(text)\n",
    "\n",
    "# 문장 토큰들을 단어 토큰화 하기\n",
    "word_tokens = [list(word_tokenize(sentence)) for sentence in sent_token]\n",
    "\n",
    "# 불용어 가져오기\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# 불용어 제거 / 정제\n",
    "result_tokens = []\n",
    "for tokens in word_tokens :\n",
    "    temp_list = []\n",
    "    for token in tokens :\n",
    "        if (token not in stop_words) & (len(token) > 2):\n",
    "            temp_list.append(token)\n",
    "    result_tokens.append(temp_list)\n",
    "    \n",
    "print(result_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 정수사전 만들기\n",
    "from "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
