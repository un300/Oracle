{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텍스트 전처리\n",
    "\n",
    "\n",
    " - 자연어 처리에 있어 텍스트 전처리는 매우 중요한 작업이다.\n",
    " - 용도에 맞게 텍스트를 처리하여야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - 목차\n",
    "\n",
    "## 1. 토큰화(Tokenization)\n",
    "   - #### 1-1 단어 토큰화(Word Tokenization)\n",
    "   - #### 1-2 토큰화 중 생기는 선택의 순간\n",
    "   - #### 1-3 토큰화에서 고려해야할 사항\n",
    "   - #### 1-4 문장 토큰화(Sentence Tokenization)\n",
    "   - #### 1-5 이진 분류기(Binary Classifier)\n",
    "\n",
    "## 2. 정제(Cleaning) and 정규화(Normalization)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 토큰화(Tokenization)\n",
    "   - 크롤링 등으로 얻어낸 텍스트를 용도에 맞게 사용하려면 다음과 같은 과정을 진행해야 한다.\n",
    "       1. 토큰화 (Tokenization)\n",
    "       2. 정제 (Cleaning)\n",
    "       3. 정규화 (Nomalization)\n",
    "   - 먼저 토큰화를 알아보자\n",
    "   - 토큰화는 간략히 말하자면, 코퍼스(Corpus)라 불리는 택스트 덩어리를 토큰(Token)이라 불리는 단위로 나누는 작업을 말한다.\n",
    "   - 토큰은 상황에 따라 의미가 다르지만, 보통 의미있는 단위로 토큰을 정의한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-1 단어 토큰화 (Word Tokenization)\n",
    "   - 토큰의 기준을 **단어**로 하는 경우 `단어 토큰화(Word Tokenization)`이라 한다. 여기서 단어는 단어단위 외에도 단어구, 의미를 갖는 문자열로도 간주되기도 한다.\n",
    "   \n",
    "##### 예를들어 아래의 입력으로 구두점(반점, 온점, 따옴표 등등..)과 같은 문자는 제외시키는 간단한 단어 토큰화 작업을 보자.\n",
    "- `Time is an illusion. Lunchtime double so!`\n",
    "- 출력 : \"Time\" / \"is\" / \"an\" / \"illustion\" / \"Lunchtime\" / \"double\" / \"so\"\n",
    "        구두점을 제외하고 whitespace를 기준으로 잘라낸 결과이다.\n",
    "        이 예제는 토큰화의 가장 기초적인 예제에 불과하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2 토큰화 중 생기는 선택의 순간\n",
    "   - 토큰화를 하다보면, 예상하지 못한 경우가 있어서 **토큰화의 기준**을 생각해봐야 하는 경우가 발생한다.\n",
    "   - 예를들어 영어의 경우 어포스트로피(')가 들어가 있는 단어는 토큰화의 기준을 어떻게 정의해야할지 정해야한다.\n",
    "   \n",
    "##### 예를들어보자\n",
    "   - `Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.`\n",
    "   \n",
    "   \n",
    "   **위 문장에서 `Don't`와 `Jone's`는 어떻게 토큰화해야하는가?**\n",
    "   - 아래의 예시를 보자.\n",
    "   제공하는 패키지마다 **토큰화의 기준**이 다름을 알 수 있다.\n",
    "  \n",
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 아래의 `word_tokenize`는 `Don't`를 `Do`와 `n't`로 구분하고 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Do', \"n't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.', 'Jone', \"'s\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize  \n",
    "print(word_tokenize(text))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 아래의 `WordPunctTokenizer`는 `Don't`를 `Don`과 `'`,  `t`로 구분하고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Don', \"'\", 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr', '.', 'Jone', \"'\", 's', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer  \n",
    "print(WordPunctTokenizer().tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### keras가 제공하는 `text_to_word_sequence`는 `'`를 구분하지 않는다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lan41\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\lan41\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\lan41\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\lan41\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\lan41\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\lan41\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\lan41\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\lan41\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\lan41\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\lan41\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\lan41\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\lan41\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"don't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', 'mr', \"jone's\", 'orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "print(text_to_word_sequence(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-3 토큰화에서 고려해야할 사항\n",
    "   - **토큰화의 작업**을 단순히 코퍼스에서 구두점을 제거하고 whitespace를 기준으로 나누는 작업이라고 간주할 수는 없다. \n",
    "   - **토큰화 작업**은 섬세한 알고리즘이 필요한데, `왜 섬세해야 하는가?` 를 알아보자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) 구두점이나 특수 문자를 단순 제외해서는 안 된다.\n",
    "   - 코퍼스에서 구두점이나 특수문자를 단순히 제외하는 것은 옳지 않다. \n",
    "   \n",
    "   \n",
    "   #### 다음예를보자\n",
    "   \n",
    "   \n",
    "   `Ph.D` / `$45.55`    >  이것들은 `.`이나 `$`가 사라지면 의미가 사라진다!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) 줄임말과 단어 내에 띄어쓰기가 있는 경우\n",
    "   - 영어권의 경우 `'`는 줄임말을 나타내기도 한다. (ex : `I'm` == `I am`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) 표준 토큰화 예제\n",
    "   - 표준으로 쓰이고 있는 토큰화 방법 중 하나인 `Penn Treebank Tokenization`의 규칙을 알아보고, 토큰화의 결과를 보도록 하자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    규칙 1. 하이푼으로 구성된 단어는 하나로 유지한다.\n",
    "    규칙 2. doesn't와 같이 아포스트로피로 '접어'가 함께하는 단어는 분리해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Starting a home-based restaurant may be an ideal. it doesn't have a food chain or restaurant of their own.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Starting', 'a', 'home-based', 'restaurant', 'may', 'be', 'an', 'ideal.', 'it', 'does', \"n't\", 'have', 'a', 'food', 'chain', 'or', 'restaurant', 'of', 'their', 'own', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer=TreebankWordTokenizer()\n",
    "print(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-4 문장 토큰화(Sentence Tokenization)\n",
    "   - 이번에는 토큰화의 단위가 문장(sentence)일 때, 어떻게 토큰화를 수행해야할지 알아보자. \n",
    "   - 이 작업은 코퍼스를 문장 단위로 구분하는 작업이다. \n",
    "   - 때로는 `문장 분류(sentence segmentation)`이라고 한다.\n",
    "   \n",
    "   \n",
    "   \n",
    "#### 그렇다면 문장을 구분하는 기준은??  :  온점`.` / 반점`,` 등으로 구분하면 되지 않을까?\n",
    "   - 아주 잘못된 생각!\n",
    "   - 다음예제를 보자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - `EX1)` IP 192.168.56.31 서버에 들어가서 로그 파일 저장해서 lan4148@naver.com으로 결과 좀 보내줘. 그러고나서 점심 먹으러 가자.\n",
    "\n",
    " - `EX2)` Since I'm actively looking for Ph.D. students, I get the same question a dozen times every year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`EX1)`의 경우 `IP 192.168.56.31`에서 반점이 포함되어 있어, 반점으로 문장을 토큰화하면 올바른 토큰화가 되지 않음!\n",
    "\n",
    "\n",
    "`EX2)`의 경우 `Ph.D.`에서 반점이 포함되어 있어, 반점으로 문장을 토큰화하면 올바른 토큰화가 이루어지지 않음!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 `nltk` 패키지에서 제공하는 `sent_tokenize`함수를 통해 실습해보자\n",
    "\n",
    "아래의 결과를 보면 성공적으로 모든 문장을 구분해 내었음을 알 수있다.\n",
    "\n",
    "`nltk`는 단순히 온점을 구분자로 하여 문장을 구분하지 않기 때문에\n",
    "\n",
    "'Ph.D.'를 문장 내의 단어로 인식하여 성공적으로 인식하는 것을 볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"His barber kept his word. But keeping such a huge secret to himself was driving him crazy. Finally, the barber went up a mountain and almost to the edge of a cliff. He dug a hole in the midst of some reeds. He looked about, to mae sure no one was near.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['His barber kept his word.', 'But keeping such a huge secret to himself was driving him crazy.', 'Finally, the barber went up a mountain and almost to the edge of a cliff.', 'He dug a hole in the midst of some reeds.', 'He looked about, to mae sure no one was near.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 한국어의 문장 토큰화를 실습해보자\n",
    "   - 한국어의 토큰화를 위한 여러가지 토큰화 도구가 존재하지만, 박상길 님이 개발한 `KSS(Korean Sentence Splitter)`를 사용해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1-5 이진 분류기(Binary Classifier)\n",
    "   - 문장 토큰화에서 예외 사항을 발생시키는 온점을 처리하기 위해서, 입력에 따라 두 개의 클래스로 분류하는 이진 분류기(Binary Classifier)를 사용하기도 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
